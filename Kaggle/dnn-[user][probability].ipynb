{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jean/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing #标准化数据模块\n",
    "########## Hyperparameter ##########\n",
    "BATCH_SIZE = 5\n",
    "EPOCH_BOUND = 1000\n",
    "EARLY_STOP_CHECK_EPOCH = 60\n",
    "TAKE_CROSS_VALIDATION = True\n",
    "LEARNING_RATE = 0.05\n",
    "CROSS_VALIDATION = 10\n",
    "########## Hyperparameter ##########\n",
    "\n",
    "def loadTrainFile():\n",
    "    tmp = np.loadtxt(\"train.csv\", dtype=np.str, delimiter=\",\")\n",
    "    userID = tmp[1:,0].astype(int)\n",
    "    item1 = tmp[1:,1].astype(int)\n",
    "    item2 = tmp[1:,2].astype(int)\n",
    "    labels = tmp[1:,3].astype(int)\n",
    "    return userID, item1, item2, labels\n",
    "def loadTestFile():\n",
    "    tmp = np.loadtxt(\"test.csv\", dtype=np.str, delimiter=\",\")\n",
    "    userID = tmp[1:,0].astype(np.float)\n",
    "    item1 = tmp[1:,1].astype(np.float)\n",
    "    item2 = tmp[1:,2].astype(np.float)\n",
    "    return userID, item1, item2\n",
    "def loadUserFile():\n",
    "    tmp = np.loadtxt(\"users.csv\", dtype=str, delimiter=\",\")\n",
    "    return tmp[1:, 1:]\n",
    "\n",
    "def loadItemFile():\n",
    "    tmp = np.loadtxt(\"items.csv\", dtype=np.str, delimiter=\",\")\n",
    "    item_dic = {}\n",
    "    for i in tmp[1:]:\n",
    "        item_dic[int(i[0])] = i[1:]\n",
    "        \n",
    "    return item_dic\n",
    "\n",
    "def dnn(x):\n",
    "    dense1 = tf.layers.dense(\n",
    "        inputs=x,\n",
    "        units=4,\n",
    "        activation=tf.nn.relu,\n",
    "        name='dense1'\n",
    "    )\n",
    "    dense2 = tf.layers.dense(\n",
    "        inputs=dense1,\n",
    "        units=4,\n",
    "        activation=tf.nn.relu,\n",
    "        name='dense2'\n",
    "    )\n",
    "    dense3 = tf.layers.dense(\n",
    "        inputs=dense2,\n",
    "        units=10,\n",
    "        activation=tf.nn.relu,\n",
    "        name='dense3'\n",
    "    )\n",
    "\n",
    "    logits = tf.layers.dense(inputs=dense3, units=10, name='logits')\n",
    "    \n",
    "    return logits\n",
    "\n",
    "# split dataset into training set and one validation set\n",
    "def split_folds(indices, Inputs, Labels, cross_validation, fold):\n",
    "    n = Inputs.shape[0]\n",
    "    if fold == cross_validation:\n",
    "        validation_size = n - (int(n/cross_validation) * (cross_validation-1))\n",
    "        X_train_idx, X_validate_idx = indices[:(n-validation_size)], indices[(n-validation_size):]\n",
    "        y_train_idx, y_validate_idx = indices[:(n-validation_size)], indices[(n-validation_size):]\n",
    "    else:\n",
    "        validation_size = int(n/cross_validation)\n",
    "        X_train_idx, X_validate_idx = np.concatenate((indices[:validation_size*(fold-1)], indices[validation_size*fold:]), axis=0), indices[(validation_size*(fold-1)):(validation_size*fold)]\n",
    "        y_train_idx, y_validate_idx = np.concatenate((indices[:validation_size*(fold-1)], indices[validation_size*fold:]), axis=0), indices[(validation_size*(fold-1)):(validation_size*fold)]\n",
    "    X_train, X_validate = np.array(Inputs[X_train_idx,:]), np.array(Inputs[X_validate_idx,:])\n",
    "    y_train, y_validate = np.array(Labels[y_train_idx]), np.array(Labels[y_validate_idx])\n",
    "    return X_train, y_train, X_validate, y_validate\n",
    "\n",
    "def train(X_train, y_train, X_validate, y_validate, optimizer, epoch_bound, stop_threshold, batch_size, testing=False):\n",
    "\n",
    "    global saver\n",
    "    global predictions\n",
    "    global loss\n",
    "    \n",
    "    early_stop = 0\n",
    "    winner_loss = np.infty\n",
    "    \n",
    "    for epoch in range(epoch_bound):\n",
    "\n",
    "        # randomize training set\n",
    "        indices_training = np.random.permutation(X_train.shape[0])\n",
    "        X_train, y_train = X_train[indices_training,:], y_train[indices_training]\n",
    "\n",
    "        # split training set into multiple mini-batches and start training\n",
    "        total_batches = int(X_train.shape[0] / batch_size)\n",
    "        for batch in range(total_batches):\n",
    "            if batch == total_batches - 1:\n",
    "                sess.run(optimizer, feed_dict={x: X_train[batch*batch_size:], \n",
    "                                               y: y_train[batch*batch_size:]})\n",
    "            else:\n",
    "                sess.run(optimizer, feed_dict={x: X_train[batch*batch_size : (batch+1)*batch_size], \n",
    "                                               y: y_train[batch*batch_size : (batch+1)*batch_size]})\n",
    "        \n",
    "        # validating\n",
    "        cur_loss = 0.0\n",
    "        total_batches = int(X_validate.shape[0] / batch_size)\n",
    "        cur_loss = sess.run(loss, feed_dict={x:X_validate,\n",
    "                                             y:y_validate})\n",
    "        \n",
    "        # If the accuracy rate does not increase for many times, it will early stop epochs-loop \n",
    "        if cur_loss < winner_loss:\n",
    "            early_stop = 0\n",
    "            winner_loss = cur_loss\n",
    "            \n",
    "            save_path = saver.save(sess, \"./saved_model/dnn.ckpt\")\n",
    "        else:\n",
    "            early_stop += 1\n",
    "        if early_stop == stop_threshold:\n",
    "            break\n",
    "    \n",
    "    saver.restore(sess, \"./saved_model/dnn.ckpt\")\n",
    "#     winner_accuracy = sess.run(accuracy, feed_dict={x:X_validate,\n",
    "#                                                     y:y_validate})\n",
    "    return winner_loss, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -1.13592367  1.13592367  0.85194275  0.85194275 -0.56796183\n",
      "  -1.98786642 -0.56796183  0.28398092  1.13592367]\n",
      " [ 0.65938047 -0.65938047  1.31876095 -0.32969024 -0.98907071 -0.65938047\n",
      "   1.64845118 -0.65938047  0.98907071 -1.31876095]\n",
      " [-0.56796183  1.13592367 -0.56796183 -0.28398092 -1.41990459  0.56796183\n",
      "   1.98786642  0.56796183 -0.28398092 -1.13592367]\n",
      " [ 0.58722022 -1.17444044  1.76166066  0.88083033  0.29361011 -1.17444044\n",
      "  -0.88083033 -1.17444044  0.88083033  0.        ]\n",
      " [ 0.          0.          0.         -1.18585412  1.18585412  0.79056942\n",
      "  -1.18585412  1.58113883  0.39528471 -1.58113883]\n",
      " [ 0.62017367 -1.24034735  1.24034735  0.31008684  0.93026051  0.\n",
      "  -2.17060786 -0.62017367  0.31008684  0.62017367]\n",
      " [ 0.         -1.17444044  1.76166066  0.88083033  0.88083033 -1.17444044\n",
      "  -0.29361011 -1.17444044  0.88083033 -0.58722022]\n",
      " [ 0.60858062 -1.21716124 -0.60858062 -0.30429031  1.52145155  0.60858062\n",
      "  -1.52145155  1.21716124 -0.91287093  0.60858062]\n",
      " [ 0.          0.31311215  0.93933644  0.62622429  0.31311215 -0.93933644\n",
      "  -0.93933644 -2.19178502  0.62622429  1.25244858]\n",
      " [ 0.          1.79284291 -1.79284291 -0.89642146 -0.29880715  0.5976143\n",
      "   0.29880715  1.19522861 -0.89642146  0.        ]\n",
      " [ 0.64549722  0.64549722 -1.29099445 -0.96824584 -0.96824584  1.29099445\n",
      "   0.96824584  1.29099445 -0.96824584 -0.64549722]\n",
      " [ 0.          0.95346259  0.         -1.43019388 -0.47673129 -0.95346259\n",
      "   2.38365647  0.         -0.47673129  0.        ]\n",
      " [-0.63245553 -0.63245553  1.8973666   0.9486833   0.31622777 -1.26491106\n",
      "   0.31622777 -1.26491106  0.9486833  -0.63245553]\n",
      " [ 0.          0.56343617  1.12687234  0.28171808 -0.84515425 -1.69030851\n",
      "   1.40859042 -1.40859042  0.84515425 -0.28171808]\n",
      " [ 1.19522861 -1.79284291  1.19522861  0.29880715  0.89642146 -0.5976143\n",
      "  -1.49403576  0.         -0.29880715  0.5976143 ]\n",
      " [ 1.          0.          1.          0.5         1.5         0.\n",
      "  -1.5         0.         -1.5        -1.        ]\n",
      " [ 0.         -1.17444044  1.76166066  0.88083033  0.88083033 -1.17444044\n",
      "  -0.88083033 -1.17444044  0.29361011  0.58722022]\n",
      " [-0.60858062 -0.60858062  1.82574186  0.91287093  0.91287093 -1.21716124\n",
      "  -0.30429031 -1.21716124  0.91287093 -0.60858062]\n",
      " [ 0.         -1.62697843  1.62697843  0.81348922  0.81348922 -1.08465229\n",
      "   0.27116307 -1.08465229  0.81348922 -0.54232614]\n",
      " [ 1.49071198 -1.49071198  0.74535599  0.372678    1.11803399  0.\n",
      "  -1.11803399  0.74535599 -1.11803399 -0.74535599]\n",
      " [ 0.          1.24034735 -1.24034735 -0.93026051 -0.93026051  1.24034735\n",
      "   0.93026051  1.24034735 -0.93026051 -0.62017367]\n",
      " [-0.67419986 -0.67419986  2.02259959  1.01129979  0.33709993 -0.67419986\n",
      "  -1.01129979 -1.34839972  0.33709993  0.67419986]\n",
      " [ 1.17444044 -0.58722022 -0.58722022 -0.29361011  0.88083033  0.58722022\n",
      "  -2.05527077  1.17444044 -0.88083033  0.58722022]\n",
      " [ 0.         -1.17444044  1.76166066  0.88083033  0.88083033 -1.17444044\n",
      "  -0.29361011 -1.17444044  0.88083033 -0.58722022]\n",
      " [ 0.64549722 -1.29099445  1.93649167  0.96824584  0.32274861 -1.29099445\n",
      "  -0.96824584 -0.64549722  0.32274861  0.        ]\n",
      " [ 0.55048188 -1.65144565  1.65144565  0.82572282  0.27524094 -0.55048188\n",
      "   0.27524094 -1.10096377  0.82572282 -1.10096377]\n",
      " [ 1.19522861 -1.19522861 -1.19522861 -0.29880715  0.29880715  1.19522861\n",
      "  -1.49403576  1.19522861 -0.29880715  0.5976143 ]\n",
      " [-0.57735027  0.57735027  0.57735027  0.28867513 -1.44337567 -0.57735027\n",
      "   2.02072594 -1.15470054  0.8660254  -0.57735027]\n",
      " [-1.38013112  1.38013112  0.69006556  1.03509834 -0.34503278 -0.69006556\n",
      "  -0.34503278 -1.38013112 -0.34503278  1.38013112]\n",
      " [ 0.48795004 -0.48795004 -0.97590007 -0.97590007  1.95180015  0.48795004\n",
      "   0.48795004 -0.48795004  0.97590007 -1.46385011]\n",
      " [ 1.34839972 -2.02259959  1.34839972 -0.33709993  0.33709993 -0.67419986\n",
      "  -1.01129979  0.          0.33709993  0.67419986]\n",
      " [-0.33709993  0.          1.68549966  1.01129979  1.01129979 -1.34839972\n",
      "  -0.67419986 -1.34839972 -0.67419986  0.67419986]\n",
      " [-0.55048188  0.55048188  1.10096377  0.27524094 -1.37620471  0.\n",
      "   1.92668659 -1.10096377  0.27524094 -1.10096377]\n",
      " [ 0.33333333 -0.33333333  2.          0.33333333  0.66666667 -1.33333333\n",
      "   0.33333333 -1.66666667  0.33333333 -0.66666667]\n",
      " [ 1.15044748 -1.91741247  1.15044748  0.38348249  1.15044748  0.38348249\n",
      "  -0.38348249  0.         -0.76696499 -1.15044748]\n",
      " [ 0.         -1.15470054  1.73205081  0.8660254   0.8660254  -1.15470054\n",
      "  -0.8660254  -1.15470054  0.8660254   0.        ]\n",
      " [ 0.          1.08465229 -1.62697843 -0.81348922 -0.81348922  0.54232614\n",
      "   1.89814151  0.54232614 -0.81348922  0.        ]\n",
      " [ 0.58722022 -1.17444044  1.76166066  0.88083033  0.29361011 -1.17444044\n",
      "  -0.88083033 -1.17444044  0.88083033  0.        ]\n",
      " [ 0.55901699 -1.67705098  1.67705098  0.83852549  0.2795085  -1.11803399\n",
      "  -0.2795085  -1.11803399  0.83852549  0.        ]\n",
      " [ 1.17444044 -1.76166066  1.17444044  0.29361011  0.88083033 -1.17444044\n",
      "  -0.88083033 -0.58722022  0.88083033  0.        ]\n",
      " [ 0.          0.          1.97814142  0.98907071  0.32969024 -0.65938047\n",
      "  -1.64845118 -0.65938047 -0.98907071  0.65938047]\n",
      " [-0.60858062  0.60858062  1.82574186  0.91287093 -0.91287093 -1.21716124\n",
      "   0.91287093 -1.21716124  0.30429031 -0.60858062]\n",
      " [ 1.17444044 -1.76166066  1.17444044  0.29361011  0.88083033 -0.58722022\n",
      "  -1.46805055 -0.58722022  0.29361011  0.58722022]\n",
      " [ 0.60858062  0.60858062 -1.82574186 -0.91287093 -0.91287093  1.21716124\n",
      "   0.91287093  1.21716124 -0.30429031 -0.60858062]\n",
      " [-0.72547625  0.72547625 -0.36273813  0.72547625 -1.4509525  -0.72547625\n",
      "   1.08821438  0.          1.81369063 -1.08821438]\n",
      " [ 1.26491106 -1.8973666   0.         -0.31622777  0.9486833   0.\n",
      "  -1.58113883  0.63245553  0.9486833   0.        ]\n",
      " [ 1.15470054 -1.15470054  0.57735027 -0.28867513  0.8660254  -0.57735027\n",
      "  -2.02072594  1.15470054 -0.28867513  0.57735027]\n",
      " [ 0.28171808 -0.84515425  1.97202659  0.          0.         -1.40859042\n",
      "   0.28171808 -1.40859042  1.12687234  0.        ]\n",
      " [ 0.64549722  0.64549722 -1.93649167 -0.96824584 -0.96824584  0.64549722\n",
      "   0.96824584  1.29099445  0.32274861 -0.64549722]\n",
      " [ 0.61429512  0.92144268  0.61429512  1.22859023  0.30714756 -1.53573779\n",
      "   0.30714756 -1.84288535  0.30714756 -0.92144268]\n",
      " [ 1.17444044 -1.76166066  1.76166066 -0.29361011  0.29361011 -0.58722022\n",
      "  -0.29361011  0.58722022  0.29361011 -1.17444044]\n",
      " [ 1.31876095 -1.31876095  1.31876095  0.32969024  0.98907071  0.\n",
      "  -1.64845118  0.         -0.98907071  0.        ]\n",
      " [ 0.56796183 -1.7038855   1.7038855   0.85194275  0.28398092 -1.13592367\n",
      "  -0.85194275 -0.56796183  0.85194275  0.        ]\n",
      " [ 0.         -1.34839972  0.67419986  1.01129979  1.01129979 -0.67419986\n",
      "  -1.68549966 -0.67419986  0.33709993  1.34839972]\n",
      " [ 0.         -1.17444044  1.76166066  0.88083033  0.88083033 -1.17444044\n",
      "  -0.29361011 -1.17444044  0.88083033 -0.58722022]\n",
      " [-0.5976143  -1.19522861  1.79284291  0.89642146  0.89642146 -0.5976143\n",
      "  -0.89642146 -1.19522861  0.89642146  0.        ]\n",
      " [-0.2795085   0.83852549  0.         -0.55901699 -1.95655948  0.55901699\n",
      "   1.67705098  0.83852549  0.         -1.11803399]\n",
      " [ 0.4472136   0.          0.          0.4472136   2.23606798 -0.4472136\n",
      "  -1.78885438  0.4472136  -0.4472136  -0.89442719]\n",
      " [ 0.         -0.33333333 -1.         -1.33333333  0.33333333  1.\n",
      "   0.          2.33333333 -0.66666667 -0.33333333]\n",
      " [-0.5976143   0.29880715  0.89642146  0.5976143  -1.49403576 -1.19522861\n",
      "   1.79284291 -0.5976143   0.89642146 -0.5976143 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jean/tensorflow/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "########### Data ###########\n",
    "user_dic = loadUserFile()\n",
    "item_dic = loadItemFile()\n",
    "userID, item1, item2, labels = loadTrainFile()\n",
    "\n",
    "X_train = user_dic.astype(int)\n",
    "y_train = np.zeros([len(user_dic),10], dtype=int)\n",
    "\n",
    "\n",
    "for idx, label in enumerate(labels):\n",
    "    if(label==0):\n",
    "        y_train[userID[idx]-1][item1[idx]-1]+=1\n",
    "        y_train[userID[idx]-1][item2[idx]-1]-=1\n",
    "    else:\n",
    "        y_train[userID[idx]-1][item1[idx]-1]-=1\n",
    "        y_train[userID[idx]-1][item2[idx]-1]+=1\n",
    "\n",
    "X_train = preprocessing.scale(X_train, axis=0, copy=False)\n",
    "y_train = preprocessing.scale(y_train, axis=1, copy=False)\n",
    "print(y_train)\n",
    "\n",
    "########### Data ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Model ###########\n",
    "x = tf.placeholder(tf.float32, [None, X_train.shape[1]], name='x')\n",
    "y = tf.placeholder(tf.float32, [None, y_train.shape[1]], name='y')\n",
    "\n",
    "logits = dnn(x)\n",
    "loss = tf.reduce_mean(tf.reduce_sum(tf.square(logits-y), axis=1)/2, axis=0, name=\"loss\") \n",
    "\n",
    "# Training iteration\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n",
    "train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "\n",
    "# # Calculate Accuracy\n",
    "# probabilities = tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "# correct_prediction = tf.equal(y, tf.argmax(probabilities,1,output_type=tf.int32))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Start training ##########\n",
      "########## Fold: 1 ##########\n",
      "validate data:  (6, 4)\n",
      "validate label:  (6, 10)\n",
      "train data:  (54, 4)\n",
      "train label:  (54, 10)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (5, 10) for Tensor 'y:0', which has shape '(?,)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2abd0f93da33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         winner_loss, epoch = train(X_train_fold, y_train_fold, X_validate_fold, y_validate_fold\n\u001b[0;32m---> 28\u001b[0;31m                                 , train_op, EPOCH_BOUND, EARLY_STOP_CHECK_EPOCH, BATCH_SIZE, testing=False)\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mwinner_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2f6ede9899e3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X_train, y_train, X_validate, y_validate, optimizer, epoch_bound, stop_threshold, batch_size, testing)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 sess.run(optimizer, feed_dict={x: X_train[batch*batch_size : (batch+1)*batch_size], \n\u001b[0;32m--> 102\u001b[0;31m                                                y: y_train[batch*batch_size : (batch+1)*batch_size]})\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# validating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1094\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1097\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (5, 10) for Tensor 'y:0', which has shape '(?,)'"
     ]
    }
   ],
   "source": [
    "########## Train ##########\n",
    "print(\"########## Start training ##########\")\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"./log\", sess.graph)\n",
    "init = tf.global_variables_initializer()\n",
    "# init saver to save model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# randomize dataset\n",
    "indices = np.random.permutation(X_train.shape[0])\n",
    "\n",
    "# start cross validation\n",
    "avg_loss = 0.0\n",
    "\n",
    "if TAKE_CROSS_VALIDATION == True:\n",
    "    for fold in range(1, CROSS_VALIDATION+1):\n",
    "        print(\"########## Fold:\", fold, \"##########\")\n",
    "        # init weights\n",
    "        sess.run(init)\n",
    "        # split inputs into training set and validation set for each fold\n",
    "        X_train_fold, y_train_fold, X_validate_fold, y_validate_fold = split_folds(indices, X_train, y_train, CROSS_VALIDATION, fold)\n",
    "        print('validate data: ', X_validate_fold.shape)\n",
    "        print('validate label: ', y_validate_fold.shape)\n",
    "        print('train data: ', X_train_fold.shape)\n",
    "        print('train label: ', y_train_fold.shape)\n",
    "\n",
    "        winner_loss, epoch = train(X_train_fold, y_train_fold, X_validate_fold, y_validate_fold\n",
    "                                , train_op, EPOCH_BOUND, EARLY_STOP_CHECK_EPOCH, BATCH_SIZE, testing=False)\n",
    "        avg_loss += winner_loss\n",
    "        \n",
    "        \n",
    "        print(\"Epoch: \", epoch, \" Loss: \", winner_loss)\n",
    "    avg_loss /= CROSS_VALIDATION\n",
    "    \n",
    "    \n",
    "    print(\"average loss: \", avg_loss)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Final Train ##########\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"./log\", sess.graph)\n",
    "init = tf.global_variables_initializer()\n",
    "# init saver to save model\n",
    "saver = tf.train.Saver()\n",
    "print(\"########## Start training ##########\")\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(50):\n",
    "\n",
    "        # randomize training set\n",
    "        indices_training = np.random.permutation(X_train.shape[0])\n",
    "        X_train, y_train = X_train[indices_training,:], y_train[indices_training]\n",
    "        \n",
    "        # split training set into multiple mini-batches and start training\n",
    "        total_batches = int(X_train.shape[0] / 1)\n",
    "        for batch in range(total_batches):\n",
    "            if batch == total_batches - 1:\n",
    "                sess.run(train_op, feed_dict={x: X_train[batch*BATCH_SIZE:], \n",
    "                                               y: y_train[batch*BATCH_SIZE:]})\n",
    "            else:\n",
    "                sess.run(train_op, feed_dict={x: X_train[batch*BATCH_SIZE : (batch+1)*BATCH_SIZE], \n",
    "                                               y: y_train[batch*BATCH_SIZE : (batch+1)*BATCH_SIZE]})\n",
    "writer.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit = 0\n",
    "user_preference = sess.run(logits, feed_dict={x:user_dic[userID-1]})\n",
    "print(user_preference.shape)\n",
    "for idx, label in enumerate(labels):\n",
    "    if(label==0 and (user_preference[idx][item1[idx]-1]>user_preference[idx][item2[idx]-1])):\n",
    "        hit+=1\n",
    "    elif(label==1 and (user_preference[idx][item1[idx]-1]<user_preference[idx][item2[idx]-1])):\n",
    "        hit+=1\n",
    "print(hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Final Train ##########\n",
    "print(\"########## Start training ##########\")\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(100):\n",
    "\n",
    "        # randomize training set\n",
    "        indices_training = np.random.permutation(X_train.shape[0])\n",
    "        X_train, y_train = X_train[indices_training,:], y_train[indices_training]\n",
    "        \n",
    "        # split training set into multiple mini-batches and start training\n",
    "        total_batches = int(X_train.shape[0] / BATCH_SIZE)\n",
    "        for batch in range(total_batches):\n",
    "            if batch == total_batches - 1:\n",
    "                sess.run(train_op, feed_dict={x: X_train[batch*BATCH_SIZE:], \n",
    "                                               y: y_train[batch*BATCH_SIZE:]})\n",
    "            else:\n",
    "                sess.run(train_op, feed_dict={x: X_train[batch*BATCH_SIZE : (batch+1)*BATCH_SIZE], \n",
    "                                               y: y_train[batch*BATCH_SIZE : (batch+1)*BATCH_SIZE]})\n",
    "        \n",
    "pridiction = tf.argmax(probabilities,axis=1,output_type=tf.int32, name=\"pridiction\")\n",
    "pridict_output = sess.run(pridiction, feed_dict={x:X_test})\n",
    "\n",
    "test_output=[['User-Item1-Item2','Preference']]\n",
    "for idx in range(pridict_output.shape[0]):\n",
    "    entry = str(int(userID[idx]))+'-'+str(int(item1[idx]))+'-'+str(int(item2[idx]))\n",
    "    value = pridict_output[idx]\n",
    "    test_output.append([entry,value])\n",
    "\n",
    "print(test_output)\n",
    "np.savetxt(\"output.csv\", np.array(test_output, dtype=np.str), fmt='%s,%s', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-virtualenv-tensorflow",
   "language": "python",
   "name": "my-virtualenv-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
