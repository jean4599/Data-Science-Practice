{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jean/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Start training ##########\n",
      "########## Fold: 1 ##########\n",
      "validate data:  (136, 12)\n",
      "validate label:  (136,)\n",
      "train data:  (1228, 12)\n",
      "train label:  (1228,)\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/dnn.ckpt\n",
      "Epoch:  76  Loss:  0.62438536  Accuracy:  0.6617647\n",
      "########## Fold: 2 ##########\n",
      "validate data:  (136, 12)\n",
      "validate label:  (136,)\n",
      "train data:  (1228, 12)\n",
      "train label:  (1228,)\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/dnn.ckpt\n",
      "Epoch:  95  Loss:  0.628346  Accuracy:  0.6764706\n",
      "########## Fold: 3 ##########\n",
      "validate data:  (136, 12)\n",
      "validate label:  (136,)\n",
      "train data:  (1228, 12)\n",
      "train label:  (1228,)\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/dnn.ckpt\n",
      "Epoch:  88  Loss:  0.6342137  Accuracy:  0.6911765\n",
      "########## Fold: 4 ##########\n",
      "validate data:  (136, 12)\n",
      "validate label:  (136,)\n",
      "train data:  (1228, 12)\n",
      "train label:  (1228,)\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/dnn.ckpt\n",
      "Epoch:  113  Loss:  0.6409118  Accuracy:  0.6397059\n",
      "########## Fold: 5 ##########\n",
      "validate data:  (136, 12)\n",
      "validate label:  (136,)\n",
      "train data:  (1228, 12)\n",
      "train label:  (1228,)\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/dnn.ckpt\n",
      "Epoch:  96  Loss:  0.65098584  Accuracy:  0.60294116\n",
      "########## Fold: 6 ##########\n",
      "validate data:  (136, 12)\n",
      "validate label:  (136,)\n",
      "train data:  (1228, 12)\n",
      "train label:  (1228,)\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/dnn.ckpt\n",
      "Epoch:  100  Loss:  0.63649035  Accuracy:  0.63235295\n",
      "########## Fold: 7 ##########\n",
      "validate data:  (136, 12)\n",
      "validate label:  (136,)\n",
      "train data:  (1228, 12)\n",
      "train label:  (1228,)\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/dnn.ckpt\n",
      "Epoch:  103  Loss:  0.6635993  Accuracy:  0.61764705\n",
      "########## Fold: 8 ##########\n",
      "validate data:  (136, 12)\n",
      "validate label:  (136,)\n",
      "train data:  (1228, 12)\n",
      "train label:  (1228,)\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/dnn.ckpt\n",
      "Epoch:  102  Loss:  0.5968402  Accuracy:  0.72794116\n",
      "########## Fold: 9 ##########\n",
      "validate data:  (136, 12)\n",
      "validate label:  (136,)\n",
      "train data:  (1228, 12)\n",
      "train label:  (1228,)\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/dnn.ckpt\n",
      "Epoch:  164  Loss:  0.6228545  Accuracy:  0.6544118\n",
      "########## Fold: 10 ##########\n",
      "validate data:  (140, 12)\n",
      "validate label:  (140,)\n",
      "train data:  (1224, 12)\n",
      "train label:  (1224,)\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/dnn.ckpt\n",
      "Epoch:  120  Loss:  0.5934871  Accuracy:  0.69285715\n",
      "average loss:  0.6292114138603211\n",
      "average accuracy:  0.6597268879413605\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "########## Hyperparameter ##########\n",
    "BATCH_SIZE = 15\n",
    "EPOCH_BOUND = 1000\n",
    "EARLY_STOP_CHECK_EPOCH = 60\n",
    "TAKE_CROSS_VALIDATION = True\n",
    "LEARNING_RATE = 0.1\n",
    "CROSS_VALIDATION = 10\n",
    "########## Hyperparameter ##########\n",
    "\n",
    "def loadTrainFile():\n",
    "    tmp = np.loadtxt(\"train.csv\", dtype=np.str, delimiter=\",\")\n",
    "    userID = tmp[1:,0].astype(np.float)\n",
    "    item1 = tmp[1:,1].astype(np.float)\n",
    "    item2 = tmp[1:,2].astype(np.float)\n",
    "    label = tmp[1:,3].astype(np.float)\n",
    "    return userID, item1, item2, label\n",
    "def loadTestFile():\n",
    "    tmp = np.loadtxt(\"test.csv\", dtype=np.str, delimiter=\",\")\n",
    "    userID = tmp[1:,0].astype(np.float)\n",
    "    item1 = tmp[1:,1].astype(np.float)\n",
    "    item2 = tmp[1:,2].astype(np.float)\n",
    "    return userID, item1, item2\n",
    "def loadUserFile():\n",
    "    tmp = np.loadtxt(\"users.csv\", dtype=np.str, delimiter=\",\")\n",
    "    user_dic = {}\n",
    "    for u in tmp[1:]:\n",
    "        user_dic[int(u[0])] = u[1:]\n",
    "    return user_dic\n",
    "\n",
    "def loadItemFile():\n",
    "    tmp = np.loadtxt(\"items.csv\", dtype=np.str, delimiter=\",\")\n",
    "    item_dic = {}\n",
    "    for i in tmp[1:]:\n",
    "        item_dic[int(i[0])] = i[1:]\n",
    "        \n",
    "    return item_dic\n",
    "\n",
    "def dnn(x):\n",
    "    dense1 = tf.layers.dense(\n",
    "        inputs=x,\n",
    "        units=10,\n",
    "        activation=tf.nn.relu,\n",
    "        name='dense1'\n",
    "    )\n",
    "    dense2 = tf.layers.dense(\n",
    "        inputs=dense1,\n",
    "        units=10,\n",
    "        activation=tf.nn.relu,\n",
    "        name='dense2'\n",
    "    )\n",
    "    dense3 = tf.layers.dense(\n",
    "        inputs=dense2,\n",
    "        units=10,\n",
    "        activation=tf.nn.relu,\n",
    "        name='dense3'\n",
    "    )\n",
    "    logits = tf.layers.dense(inputs=dense3, units=2, name='logits')\n",
    "    \n",
    "    return logits\n",
    "\n",
    "# split dataset into training set and one validation set\n",
    "def split_folds(indices, Inputs, Labels, cross_validation, fold):\n",
    "    n = Inputs.shape[0]\n",
    "    if fold == cross_validation:\n",
    "        validation_size = n - (int(n/cross_validation) * (cross_validation-1))\n",
    "        X_train_idx, X_validate_idx = indices[:(n-validation_size)], indices[(n-validation_size):]\n",
    "        y_train_idx, y_validate_idx = indices[:(n-validation_size)], indices[(n-validation_size):]\n",
    "    else:\n",
    "        validation_size = int(n/cross_validation)\n",
    "        X_train_idx, X_validate_idx = np.concatenate((indices[:validation_size*(fold-1)], indices[validation_size*fold:]), axis=0), indices[(validation_size*(fold-1)):(validation_size*fold)]\n",
    "        y_train_idx, y_validate_idx = np.concatenate((indices[:validation_size*(fold-1)], indices[validation_size*fold:]), axis=0), indices[(validation_size*(fold-1)):(validation_size*fold)]\n",
    "    X_train, X_validate = np.array(Inputs[X_train_idx,:]), np.array(Inputs[X_validate_idx,:])\n",
    "    y_train, y_validate = np.array(Labels[y_train_idx]), np.array(Labels[y_validate_idx])\n",
    "    return X_train, y_train, X_validate, y_validate\n",
    "\n",
    "def train(X_train, y_train, X_validate, y_validate, optimizer, epoch_bound, stop_threshold, batch_size, testing=False):\n",
    "\n",
    "    global saver\n",
    "    global predictions\n",
    "    global loss\n",
    "    \n",
    "    early_stop = 0\n",
    "    winner_loss = np.infty\n",
    "    \n",
    "    for epoch in range(epoch_bound):\n",
    "\n",
    "        # randomize training set\n",
    "        indices_training = np.random.permutation(X_train.shape[0])\n",
    "        X_train, y_train = X_train[indices_training,:], y_train[indices_training]\n",
    "\n",
    "        # split training set into multiple mini-batches and start training\n",
    "        total_batches = int(X_train.shape[0] / batch_size)\n",
    "        for batch in range(total_batches):\n",
    "            if batch == total_batches - 1:\n",
    "                sess.run(optimizer, feed_dict={x: X_train[batch*batch_size:], \n",
    "                                               y: y_train[batch*batch_size:]})\n",
    "            else:\n",
    "                sess.run(optimizer, feed_dict={x: X_train[batch*batch_size : (batch+1)*batch_size], \n",
    "                                               y: y_train[batch*batch_size : (batch+1)*batch_size]})\n",
    "        \n",
    "        # validating\n",
    "        cur_loss = 0.0\n",
    "        total_batches = int(X_validate.shape[0] / batch_size)\n",
    "        cur_loss = sess.run(loss, feed_dict={x:X_validate,\n",
    "                                             y:y_validate})\n",
    "        \n",
    "        # If the accuracy rate does not increase for many times, it will early stop epochs-loop \n",
    "        if cur_loss < winner_loss:\n",
    "            early_stop = 0\n",
    "            winner_loss = cur_loss\n",
    "            \n",
    "            save_path = saver.save(sess, \"./saved_model/dnn.ckpt\")\n",
    "        else:\n",
    "            early_stop += 1\n",
    "        if early_stop == stop_threshold:\n",
    "            break\n",
    "    \n",
    "    saver.restore(sess, \"./saved_model/dnn.ckpt\")\n",
    "    winner_accuracy = sess.run(accuracy, feed_dict={x:X_validate,\n",
    "                                                    y:y_validate})\n",
    "    return winner_loss, winner_accuracy, epoch\n",
    "########### Data ###########\n",
    "user_dic = loadUserFile()\n",
    "item_dic = loadItemFile()\n",
    "#train\n",
    "userID, item1, item2, label = loadTrainFile()\n",
    "\n",
    "user_X = []\n",
    "item1_X = []\n",
    "item2_X = []\n",
    "\n",
    "for u in userID:\n",
    "    user_X.append(user_dic[int(u)])\n",
    "for i in item1:\n",
    "    item1_X.append(item_dic[int(i)])\n",
    "for i in item2:\n",
    "    item2_X.append(item_dic[int(i)])\n",
    "\n",
    "X_train = np.concatenate([user_X, item1_X, item2_X], axis=1)\n",
    "X_train = X_train.astype(float)\n",
    "y_train = label.astype(int)\n",
    "\n",
    "#test\n",
    "userID, item1, item2 = loadTestFile()\n",
    "user_X = []\n",
    "item1_X = []\n",
    "item2_X = []\n",
    "\n",
    "for u in userID:\n",
    "    user_X.append(user_dic[int(u)])\n",
    "for i in item1:\n",
    "    item1_X.append(item_dic[int(i)])\n",
    "for i in item2:\n",
    "    item2_X.append(item_dic[int(i)])\n",
    "\n",
    "X_test = np.concatenate([user_X, item1_X, item2_X], axis=1)\n",
    "X_test = X_test.astype(float)\n",
    "########### Data ###########\n",
    "\n",
    "########### Model ###########\n",
    "x = tf.placeholder(tf.float32, [None, X_train.shape[1]], name='x')\n",
    "y = tf.placeholder(tf.int32, [None], name='y')\n",
    "onehot_y = tf.one_hot(indices=tf.cast(y,tf.int32), depth=2)\n",
    "\n",
    "logits = dnn(x)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=onehot_y, logits=logits, name=\"loss\"))\n",
    "\n",
    "# Training iteration\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n",
    "train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "\n",
    "# Calculate Accuracy\n",
    "probabilities = tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "correct_prediction = tf.equal(y, tf.argmax(probabilities,1,output_type=tf.int32))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "\n",
    "########## Train ##########\n",
    "print(\"########## Start training ##########\")\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"./log\", sess.graph)\n",
    "init = tf.global_variables_initializer()\n",
    "# init saver to save model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# randomize dataset\n",
    "indices = np.random.permutation(X_train.shape[0])\n",
    "\n",
    "# start cross validation\n",
    "avg_accuracy = 0.0\n",
    "avg_loss = 0.0\n",
    "\n",
    "if TAKE_CROSS_VALIDATION == True:\n",
    "    for fold in range(1, CROSS_VALIDATION+1):\n",
    "        print(\"########## Fold:\", fold, \"##########\")\n",
    "        # init weights\n",
    "        sess.run(init)\n",
    "        # split inputs into training set and validation set for each fold\n",
    "        X_train_fold, y_train_fold, X_validate_fold, y_validate_fold = split_folds(indices, X_train, y_train, CROSS_VALIDATION, fold)\n",
    "        print('validate data: ', X_validate_fold.shape)\n",
    "        print('validate label: ', y_validate_fold.shape)\n",
    "        print('train data: ', X_train_fold.shape)\n",
    "        print('train label: ', y_train_fold.shape)\n",
    "\n",
    "        winner_loss, winner_accuracy, epoch = train(X_train_fold, y_train_fold, X_validate_fold, y_validate_fold\n",
    "                                , train_op, EPOCH_BOUND, EARLY_STOP_CHECK_EPOCH, BATCH_SIZE, testing=False)\n",
    "        avg_loss += winner_loss\n",
    "        avg_accuracy += winner_accuracy\n",
    "        \n",
    "        print(\"Epoch: \", epoch, \" Loss: \", winner_loss, \" Accuracy: \", winner_accuracy)\n",
    "    avg_loss /= CROSS_VALIDATION\n",
    "    avg_accuracy /=CROSS_VALIDATION\n",
    "    \n",
    "    print(\"average loss: \", avg_loss)\n",
    "    print(\"average accuracy: \", avg_accuracy)\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Final Train ##########\n",
    "print(\"########## Start training ##########\")\n",
    "sess.run(init)\n",
    "\n",
    "# randomize dataset\n",
    "indices = np.random.permutation(X_train.shape[0])\n",
    "\n",
    "for epoch in range(100):\n",
    "\n",
    "        # randomize training set\n",
    "        indices_training = np.random.permutation(X_train.shape[0])\n",
    "        X_train, y_train = X_train[indices_training,:], y_train[indices_training]\n",
    "\n",
    "        # split training set into multiple mini-batches and start training\n",
    "        total_batches = int(X_train.shape[0] / batch_size)\n",
    "        for batch in range(total_batches):\n",
    "            if batch == total_batches - 1:\n",
    "                sess.run(optimizer, feed_dict={x: X_train[batch*batch_size:], \n",
    "                                               y: y_train[batch*batch_size:]})\n",
    "            else:\n",
    "                sess.run(optimizer, feed_dict={x: X_train[batch*batch_size : (batch+1)*batch_size], \n",
    "                                               y: y_train[batch*batch_size : (batch+1)*batch_size]})\n",
    "        \n",
    "\n",
    "pridict = tf.argmax(input=probabilities, axis=1)\n",
    "pridict_output = sess.run(pridict, feed_dict={x:X_test})\n",
    "test_output = np.concatenate((X_test,pridict_output),axis=1)\n",
    "np.savetxt(\"output.csv\", test_output, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-virtualenv-tensorflow",
   "language": "python",
   "name": "my-virtualenv-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
